{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torchaudio\nfrom datasets import Dataset\nfrom transformers import (\n    AutoProcessor, \n    AutoModelForAudioClassification, \n    TrainingArguments, \n    Trainer\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:37:45.302822Z","iopub.execute_input":"2025-07-25T10:37:45.303333Z","iopub.status.idle":"2025-07-25T10:37:46.023751Z","shell.execute_reply.started":"2025-07-25T10:37:45.303307Z","shell.execute_reply":"2025-07-25T10:37:46.023159Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def extract_emotion(filename):\n    code = int(filename.split(\"-\")[2])\n    emotion_map = {\n        1: \"neutral\",\n        2: \"calm\",\n        3: \"happy\",\n        4: \"sad\",\n        5: \"angry\",\n        6: \"fearful\",\n        7: \"disgust\",\n        8: \"surprised\"\n    }\n    return emotion_map[code]\n\naudio_root = \"/kaggle/input/ravdess-emotional-speech-audio\"\n\ndata = []\nfor actor_dir in sorted(os.listdir(audio_root)):\n    actor_path = os.path.join(audio_root, actor_dir)\n    if os.path.isdir(actor_path):\n        for fname in os.listdir(actor_path):\n            if fname.endswith(\".wav\"):\n                full_path = os.path.join(actor_path, fname)\n                emotion = extract_emotion(fname)\n                data.append({\"path\": full_path, \"label\": emotion})\n\ndf = pd.DataFrame(data)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\ndf.head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:37:46.024861Z","iopub.execute_input":"2025-07-25T10:37:46.025211Z","iopub.status.idle":"2025-07-25T10:37:46.301479Z","shell.execute_reply.started":"2025-07-25T10:37:46.025186Z","shell.execute_reply":"2025-07-25T10:37:46.300743Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                path      label\n0  /kaggle/input/ravdess-emotional-speech-audio/A...    fearful\n1  /kaggle/input/ravdess-emotional-speech-audio/A...  surprised\n2  /kaggle/input/ravdess-emotional-speech-audio/A...        sad\n3  /kaggle/input/ravdess-emotional-speech-audio/A...    fearful\n4  /kaggle/input/ravdess-emotional-speech-audio/A...        sad\n5  /kaggle/input/ravdess-emotional-speech-audio/A...    disgust\n6  /kaggle/input/ravdess-emotional-speech-audio/A...    neutral\n7  /kaggle/input/ravdess-emotional-speech-audio/A...       calm\n8  /kaggle/input/ravdess-emotional-speech-audio/A...    disgust\n9  /kaggle/input/ravdess-emotional-speech-audio/A...       calm","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>fearful</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>surprised</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>sad</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>fearful</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>sad</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>disgust</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>calm</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>disgust</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>/kaggle/input/ravdess-emotional-speech-audio/A...</td>\n      <td>calm</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from datasets import Dataset\n\ndataset = Dataset.from_pandas(df)\ndataset = dataset.class_encode_column(\"label\")\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\n\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:37:49.558845Z","iopub.execute_input":"2025-07-25T10:37:49.559562Z","iopub.status.idle":"2025-07-25T10:37:49.601086Z","shell.execute_reply.started":"2025-07-25T10:37:49.559537Z","shell.execute_reply":"2025-07-25T10:37:49.600475Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Casting to class labels:   0%|          | 0/1440 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4ec05401d2c4988bfeece7a94732719"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['path', 'label'],\n        num_rows: 1152\n    })\n    test: Dataset({\n        features: ['path', 'label'],\n        num_rows: 288\n    })\n})"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torchaudio\nimport torchaudio.transforms as T\n\ndef preprocess(example):\n    try:\n        waveform, sr = torchaudio.load(example[\"path\"])\n        waveform = waveform.mean(dim=0)  \n        if sr != 16000:\n            resampler = T.Resample(orig_freq=sr, new_freq=16000)\n            waveform = resampler(waveform)\n        \n        input_values = waveform.tolist()\n        \n       \n        if not isinstance(input_values, list):\n            raise ValueError(\"Not a list\")\n\n        return {\"input_values\": input_values, \"label\": example[\"label\"]}\n    \n    except Exception as e:\n        \n        return {\"input_values\": None, \"label\": None}\ndataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n\ndataset = dataset.filter(lambda example: example[\"input_values\"] is not None)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T10:37:51.739303Z","iopub.execute_input":"2025-07-25T10:37:51.739579Z","iopub.status.idle":"2025-07-25T10:39:25.061300Z","shell.execute_reply.started":"2025-07-25T10:37:51.739559Z","shell.execute_reply":"2025-07-25T10:39:25.060742Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d141f9841d4ec896110ebf6a6f7356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/288 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8f1142c730e4f90a2cc81c32c515712"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6cd5f64f3346e3ab585c2e1ec940fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/288 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f910896c291c4750b1e56ef52ba6f142"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import (\n    AutoModelForAudioClassification,\n    TrainingArguments,\n    Trainer\n)\nimport time\nimport numpy as np\nimport evaluate\n\n\n\nclass CustomAudioCollator:\n    def __init__(self, padding_value=0.0):\n        self.padding_value = padding_value\n\n    def __call__(self, features):\n        input_values = [torch.tensor(f[\"input_values\"]) if not isinstance(f[\"input_values\"], torch.Tensor) else f[\"input_values\"] for f in features]\n        labels = torch.tensor([f[\"label\"] for f in features])\n        padded_inputs = pad_sequence(input_values, batch_first=True, padding_value=self.padding_value)\n        return {\"input_values\": padded_inputs, \"labels\": labels}\n\n\nmodel = AutoModelForAudioClassification.from_pretrained(\n    \"superb/wav2vec2-base-superb-er\",\n    num_labels=dataset[\"train\"].features[\"label\"].num_classes,\n    ignore_mismatched_sizes=True\n)\n\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./wav2vec2_ravdess\",\n    do_train=True,\n    do_eval=True,      \n    logging_strategy=\"epoch\",         \n    logging_dir=\"./logs\",\n    disable_tqdm=False,              \n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=20,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n\n\naccuracy_metric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    if isinstance(eval_pred, tuple):\n        logits, labels = eval_pred\n    else:\n        logits, labels = eval_pred.predictions, eval_pred.label_ids\n\n    if isinstance(logits, tuple):\n        logits = logits[0]\n\n    predictions = np.argmax(logits, axis=-1)\n    return accuracy_metric.compute(predictions=predictions, references=labels)\n\n\n\ncollator = CustomAudioCollator()\ntrain_dataset = dataset[\"train\"]\neval_dataset = dataset[\"test\"]\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=collator,\n    compute_metrics=compute_metrics,\n)\n\n\nstart_train = time.time()\ntrain_result = trainer.train()\nend_train = time.time()\ntrain_time = end_train - start_train\n\nprint(f\"\\nToplam eğitim süresi: {train_time:.2f} saniye\")\nprint(f\"Eğitim doğruluğu: {train_result.metrics.get('train_accuracy', 'Eğitim loglarında')}\")\n\n\n\nstart_test = time.time()\ntest_metrics = trainer.evaluate()\nend_test = time.time()\ntest_time = end_test - start_test\n\nprint(f\"Test doğruluğu: {test_metrics['eval_accuracy']:.4f}\")\nprint(f\"Toplam test süresi: {test_time:.2f} saniye\")\n\n\nmodel.save_pretrained(\"./wav2vec2_ravdess/final_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T12:33:31.831945Z","iopub.execute_input":"2025-07-25T12:33:31.832203Z","iopub.status.idle":"2025-07-25T13:03:27.528203Z","shell.execute_reply.started":"2025-07-25T12:33:31.832185Z","shell.execute_reply":"2025-07-25T13:03:27.527367Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n  warnings.warn(\nSome weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-er and are newly initialized because the shapes did not match:\n- classifier.weight: found shape torch.Size([4, 256]) in the checkpoint and torch.Size([8, 256]) in the model instantiated\n- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([8]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1440' max='1440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1440/1440 29:33, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>72</td>\n      <td>1.998200</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>1.723800</td>\n    </tr>\n    <tr>\n      <td>216</td>\n      <td>1.558300</td>\n    </tr>\n    <tr>\n      <td>288</td>\n      <td>1.376200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.211700</td>\n    </tr>\n    <tr>\n      <td>432</td>\n      <td>1.115300</td>\n    </tr>\n    <tr>\n      <td>504</td>\n      <td>1.005400</td>\n    </tr>\n    <tr>\n      <td>576</td>\n      <td>0.849700</td>\n    </tr>\n    <tr>\n      <td>648</td>\n      <td>0.775400</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.642200</td>\n    </tr>\n    <tr>\n      <td>792</td>\n      <td>0.561800</td>\n    </tr>\n    <tr>\n      <td>864</td>\n      <td>0.487900</td>\n    </tr>\n    <tr>\n      <td>936</td>\n      <td>0.418600</td>\n    </tr>\n    <tr>\n      <td>1008</td>\n      <td>0.387000</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.320800</td>\n    </tr>\n    <tr>\n      <td>1152</td>\n      <td>0.286300</td>\n    </tr>\n    <tr>\n      <td>1224</td>\n      <td>0.248400</td>\n    </tr>\n    <tr>\n      <td>1296</td>\n      <td>0.239500</td>\n    </tr>\n    <tr>\n      <td>1368</td>\n      <td>0.211800</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.199100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nToplam eğitim süresi: 1776.22 saniye\nEğitim doğruluğu: Eğitim loglarında\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18/18 00:11]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Test doğruluğu: 0.8681\nToplam test süresi: 15.43 saniye\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}